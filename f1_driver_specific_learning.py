"""
Created on Tue Jun 24 10:02:18 2025

@author: sid

FIXED F1 Driver-Specific Learning Pipeline
Fixed import issues and added proper execution
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GroupKFold
from sklearn.metrics import classification_report, confusion_matrix
import time
from typing import Dict, List, Tuple, Optional
import pickle
import json
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# FIXED IMPORT - Use the correct module name
# If the file is named 'f1_transformer_time2vec_embedding.py', import from that:
try:
    from f1_transformer_time2vec_embedding import (
        AdvancedF1TransformerModel, 
        RacingFeatureEngineer, 
        create_strategic_labels,
        evaluate_strategic_outcome,
        evaluate_energy_efficiency
    )
    print("âœ… Successfully imported from f1_transformer_time2vec_embedding")
except ImportError:
    print("âŒ Could not import from f1_transformer_time2vec_embedding")
    print("   Make sure the file is in the same directory!")
    print("   Alternative: Run this code in the same file as the model definitions")
    
    # Fallback: Define minimal versions here for demo
    class AdvancedF1TransformerModel:
        def __init__(self, **kwargs):
            print("Using fallback model class - import the real one!")
    
    class RacingFeatureEngineer:
        def __init__(self):
            print("Using fallback feature engineer - import the real one!")
    
    def create_strategic_labels(df, driver_style='balanced'):
        return np.random.randint(0, 3, len(df))
    
    def evaluate_strategic_outcome(decisions, actual_outcomes):
        return {'overall_accuracy': 0.75}
    
    def evaluate_energy_efficiency(ers_usage, strategic_decisions):
        return {'overall_ers_usage': 0.65}

class DriverSpecificLearning:
    """
    Learn driver-specific patterns from historical race data
    Captures unique racing styles of F1 masters
    """
    def __init__(self):
        self.driver_patterns = {}
        self.driver_statistics = {}
        
    def analyze_driver_patterns(self, driver_data: Dict[str, List]) -> Dict:
        """
        Analyze and learn driver-specific racing patterns
        """
        patterns = {}
        
        for driver_id, sessions in driver_data.items():
            print(f"ðŸŽï¸  Analyzing {driver_id.upper()} racing patterns...")
            
            # Aggregate driver statistics
            overtaking_attempts = []
            success_rates = []
            aggression_scores = []
            energy_efficiency = []
            
            for session_data in sessions:
                # Extract driver metrics
                session_stats = self._extract_session_statistics(session_data)
                
                overtaking_attempts.append(session_stats['overtaking_attempts'])
                success_rates.append(session_stats['success_rate'])
                aggression_scores.append(session_stats['aggression_score'])
                energy_efficiency.append(session_stats['energy_efficiency'])
            
            # Create driver profile
            driver_profile = {
                'avg_overtaking_attempts': np.mean(overtaking_attempts),
                'avg_success_rate': np.mean(success_rates),
                'aggression_factor': np.mean(aggression_scores),
                'energy_efficiency': np.mean(energy_efficiency),
                'consistency': 1.0 - np.std(success_rates),  # Lower std = more consistent
                'risk_tolerance': np.mean(aggression_scores) * np.mean(overtaking_attempts)
            }
            
            patterns[driver_id] = driver_profile
            
            print(f"   ðŸ“Š {driver_id}: "
                  f"Aggression={driver_profile['aggression_factor']:.2f}, "
                  f"Success={driver_profile['avg_success_rate']:.2f}, "
                  f"Efficiency={driver_profile['energy_efficiency']:.2f}")
        
        self.driver_patterns = patterns
        return patterns
    
    def _extract_session_statistics(self, session_data: Dict) -> Dict:
        """Extract racing statistics from a session"""
        df = session_data.get('telemetry', pd.DataFrame())
        
        if df.empty:
            return {
                'overtaking_attempts': np.random.randint(0, 5),
                'success_rate': np.random.uniform(0.3, 0.8),
                'aggression_score': np.random.uniform(0.4, 0.9),
                'energy_efficiency': np.random.uniform(0.5, 0.85)
            }
        
        # Count overtaking attempts (high throttle + close gap)
        overtaking_mask = (
            (df.get('throttle', pd.Series([0]*len(df))) > 0.8) & 
            (df.get('gap_ahead', pd.Series([999]*len(df))) < 1.0) &
            (df.get('speed', pd.Series([0]*len(df))) > 200)
        )
        overtaking_attempts = overtaking_mask.sum()
        
        # Estimate success rate (simplified)
        position_changes = abs(df.get('position', pd.Series([10]*len(df))).diff().fillna(0)).sum()
        success_rate = min(position_changes / max(overtaking_attempts, 1), 1.0)
        
        # Aggression score (based on throttle usage and braking patterns)
        throttle_mean = df.get('throttle', pd.Series([0]*len(df))).mean()
        brake_mean = df.get('brake', pd.Series([0]*len(df))).mean()
        aggression_score = throttle_mean * 0.6 + (1 - brake_mean) * 0.4
        
        # Energy efficiency (ERS usage optimization)
        ers_usage = df.get('ers', pd.Series([0]*len(df)))
        energy_efficiency = 1.0 - (ers_usage.std() / (ers_usage.mean() + 0.1))
        
        return {
            'overtaking_attempts': int(overtaking_attempts),
            'success_rate': float(success_rate),
            'aggression_score': float(aggression_score),
            'energy_efficiency': float(energy_efficiency)
        }
    
    def get_driver_context(self, driver_id: str) -> Dict:
        """Get driver-specific context for model training"""
        if driver_id not in self.driver_patterns:
            # Default profile for unknown drivers
            return {
                'aggression_factor': 0.7,
                'risk_tolerance': 0.5,
                'energy_efficiency': 0.6,
                'consistency': 0.7
            }
        
        return self.driver_patterns[driver_id]

def generate_demo_driver_data() -> Dict[str, List]:
    """
    Generate demo driver data for testing
    """
    print("ðŸŽï¸  Generating demo driver data...")
    
    drivers = ['hamilton', 'verstappen', 'leclerc', 'russell', 'sainz']
    driver_data = {}
    
    for driver in drivers:
        sessions = []
        for session_idx in range(5):  # 5 sessions per driver
            # Generate telemetry data
            seq_len = np.random.randint(30, 70)
            telemetry_df = pd.DataFrame({
                'speed': np.random.normal(250, 30, seq_len).clip(50, 320),
                'throttle': np.random.beta(2, 1, seq_len),
                'brake': np.random.beta(1, 4, seq_len),
                'ers': np.random.beta(1.5, 1.5, seq_len),
                'gear': np.random.randint(1, 8, seq_len),
                'gap_ahead': np.random.exponential(1.5, seq_len).clip(0.1, 10),
                'gap_behind': np.random.exponential(1.5, seq_len).clip(0.1, 10),
                'position': np.random.randint(1, 20, seq_len),
                'drs_available': np.random.choice([0, 1], seq_len, p=[0.7, 0.3])
            })
            
            session_data = {
                'driver_id': driver,
                'session_id': f'{driver}_session_{session_idx}',
                'telemetry': telemetry_df,
                'track_id': np.random.choice(['silverstone', 'monza', 'spa', 'monaco']),
                'race_id': f'race_{session_idx}'
            }
            sessions.append(session_data)
        
        driver_data[driver] = sessions
    
    print(f"   âœ… Generated data for {len(drivers)} drivers")
    return driver_data

def run_driver_analysis_demo():
    """
    Run the complete driver analysis demonstration
    """
    print("ðŸ F1 Driver-Specific Learning Demo")
    print("=" * 50)
    
    # Generate demo data
    driver_data = generate_demo_driver_data()
    
    # Initialize driver learning system
    print("\nðŸ§  Initializing Driver Learning System...")
    driver_learner = DriverSpecificLearning()
    
    # Analyze driver patterns
    print("\nðŸ“Š Analyzing Driver Patterns...")
    driver_patterns = driver_learner.analyze_driver_patterns(driver_data)
    
    # Display results
    print("\nðŸ† Driver Profile Analysis:")
    print("-" * 60)
    for driver, profile in driver_patterns.items():
        print(f"ðŸŽï¸  {driver.upper()}:")
        print(f"   Aggression Factor: {profile['aggression_factor']:.3f}")
        print(f"   Success Rate: {profile['avg_success_rate']:.3f}")
        print(f"   Energy Efficiency: {profile['energy_efficiency']:.3f}")
        print(f"   Consistency: {profile['consistency']:.3f}")
        print(f"   Risk Tolerance: {profile['risk_tolerance']:.3f}")
        print()
    
    # Compare drivers
    print("ðŸ“ˆ Driver Comparison:")
    print("-" * 40)
    
    # Most aggressive driver
    most_aggressive = max(driver_patterns.items(), 
                         key=lambda x: x[1]['aggression_factor'])
    print(f"Most Aggressive: {most_aggressive[0].upper()} "
          f"({most_aggressive[1]['aggression_factor']:.3f})")
    
    # Most successful driver
    most_successful = max(driver_patterns.items(), 
                         key=lambda x: x[1]['avg_success_rate'])
    print(f"Highest Success Rate: {most_successful[0].upper()} "
          f"({most_successful[1]['avg_success_rate']:.3f})")
    
    # Most consistent driver
    most_consistent = max(driver_patterns.items(), 
                         key=lambda x: x[1]['consistency'])
    print(f"Most Consistent: {most_consistent[0].upper()} "
          f"({most_consistent[1]['consistency']:.3f})")
    
    # Most energy efficient
    most_efficient = max(driver_patterns.items(), 
                        key=lambda x: x[1]['energy_efficiency'])
    print(f"Most Energy Efficient: {most_efficient[0].upper()} "
          f"({most_efficient[1]['energy_efficiency']:.3f})")
    
    # Create visualization
    print("\nðŸ“Š Creating Driver Profile Visualization...")
    create_driver_visualization(driver_patterns)
    
    # Test driver context retrieval
    print("\nðŸ” Testing Driver Context Retrieval:")
    test_drivers = ['verstappen', 'hamilton', 'unknown_driver']
    for driver in test_drivers:
        context = driver_learner.get_driver_context(driver)
        print(f"   {driver}: {context}")
    
    print("\nâœ… Driver analysis demo completed!")
    return driver_learner, driver_patterns

def create_driver_visualization(driver_patterns: Dict):
    """
    Create comprehensive driver profile visualizations
    """
    try:
        drivers = list(driver_patterns.keys())
        
        # Extract metrics
        aggression = [driver_patterns[d]['aggression_factor'] for d in drivers]
        success_rate = [driver_patterns[d]['avg_success_rate'] for d in drivers]
        energy_efficiency = [driver_patterns[d]['energy_efficiency'] for d in drivers]
        consistency = [driver_patterns[d]['consistency'] for d in drivers]
        risk_tolerance = [driver_patterns[d]['risk_tolerance'] for d in drivers]
        
        # Create subplot figure
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('F1 Driver Profile Analysis', fontsize=16, fontweight='bold')
        
        # 1. Aggression Factor
        axes[0, 0].bar(drivers, aggression, color='red', alpha=0.7)
        axes[0, 0].set_title('Aggression Factor')
        axes[0, 0].set_ylabel('Aggression Score')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. Success Rate
        axes[0, 1].bar(drivers, success_rate, color='green', alpha=0.7)
        axes[0, 1].set_title('Success Rate')
        axes[0, 1].set_ylabel('Success Percentage')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. Energy Efficiency
        axes[0, 2].bar(drivers, energy_efficiency, color='blue', alpha=0.7)
        axes[0, 2].set_title('Energy Efficiency')
        axes[0, 2].set_ylabel('Efficiency Score')
        axes[0, 2].tick_params(axis='x', rotation=45)
        
        # 4. Consistency
        axes[1, 0].bar(drivers, consistency, color='orange', alpha=0.7)
        axes[1, 0].set_title('Consistency')
        axes[1, 0].set_ylabel('Consistency Score')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 5. Risk Tolerance
        axes[1, 1].bar(drivers, risk_tolerance, color='purple', alpha=0.7)
        axes[1, 1].set_title('Risk Tolerance')
        axes[1, 1].set_ylabel('Risk Score')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        # 6. Radar Chart (Overall Profile)
        theta = np.linspace(0, 2 * np.pi, 5, endpoint=False)
        metrics = ['Aggression', 'Success', 'Efficiency', 'Consistency', 'Risk']
        
        ax_radar = axes[1, 2]
        ax_radar.set_theta_offset(np.pi / 2)
        ax_radar.set_theta_direction(-1)
        
        # Plot each driver
        colors = ['red', 'blue', 'green', 'orange', 'purple']
        for i, driver in enumerate(drivers):
            values = [
                aggression[i], success_rate[i], energy_efficiency[i],
                consistency[i], risk_tolerance[i]
            ]
            values += values[:1]  # Complete the circle
            theta_plot = np.concatenate([theta, [theta[0]]])
            
            ax_radar.plot(theta_plot, values, 'o-', linewidth=2, 
                         label=driver.upper(), color=colors[i % len(colors)])
            ax_radar.fill(theta_plot, values, alpha=0.25, color=colors[i % len(colors)])
        
        ax_radar.set_xticks(theta)
        ax_radar.set_xticklabels(metrics)
        ax_radar.set_ylim(0, 1)
        ax_radar.set_title('Driver Profile Comparison')
        ax_radar.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
        ax_radar.grid(True)
        
        plt.tight_layout()
        plt.show()
        
        print("   âœ… Visualization created successfully!")
        
    except Exception as e:
        print(f"   âŒ Could not create visualization: {e}")
        print("   (This is normal if matplotlib is not available)")

class RaceByRaceValidator:
    """
    Simplified race validation for demo purposes
    """
    def __init__(self, n_splits: int = 3):
        self.n_splits = n_splits
        self.validation_results = []
        
    def validate_model_simple(self, driver_data: Dict) -> Dict:
        """
        Simplified validation for demo
        """
        print(f"ðŸ”„ Running {self.n_splits}-fold validation...")
        
        # Simulate validation results
        fold_results = []
        for fold in range(self.n_splits):
            print(f"   Fold {fold + 1}/{self.n_splits}...")
            
            # Simulate metrics
            fold_metrics = {
                'accuracy': np.random.uniform(0.7, 0.9),
                'precision': np.random.uniform(0.65, 0.85),
                'recall': np.random.uniform(0.7, 0.9),
                'f1_score': np.random.uniform(0.68, 0.88)
            }
            fold_results.append(fold_metrics)
            
            print(f"     Accuracy: {fold_metrics['accuracy']:.3f}")
        
        # Aggregate results
        aggregated = {}
        metrics = ['accuracy', 'precision', 'recall', 'f1_score']
        for metric in metrics:
            values = [fold[metric] for fold in fold_results]
            aggregated[f'{metric}_mean'] = np.mean(values)
            aggregated[f'{metric}_std'] = np.std(values)
        
        print(f"\nðŸ“Š Cross-Validation Results:")
        for metric in metrics:
            mean_val = aggregated[f'{metric}_mean']
            std_val = aggregated[f'{metric}_std']
            print(f"   {metric.title()}: {mean_val:.3f} Â± {std_val:.3f}")
        
        return aggregated

def run_full_demo():
    """
    Run the complete demo including both driver analysis and validation
    """
    print("ðŸš€ Running Complete F1 AI Training Demo")
    print("=" * 60)
    
    # Run driver analysis
    driver_learner, driver_patterns = run_driver_analysis_demo()
    
    # Run simplified validation
    print("\n" + "=" * 60)
    print("ðŸ§ª Model Validation Demo")
    validator = RaceByRaceValidator(n_splits=3)
    
    # Generate some race data for validation
    race_data = {}
    for driver in driver_patterns.keys():
        race_data[driver] = driver_learner.driver_patterns[driver]
    
    validation_results = validator.validate_model_simple(race_data)
    
    # Summary
    print("\n" + "=" * 60)
    print("ðŸ“‹ Demo Summary")
    print(f"âœ… Analyzed {len(driver_patterns)} drivers")
    print(f"âœ… Completed {validator.n_splits}-fold cross-validation")
    print(f"âœ… Average model accuracy: {validation_results['accuracy_mean']:.3f}")
    
    # Next steps
    print("\nðŸŽ¯ To fix your original issue:")
    print("1. Save the first code as 'f1_transformer_time2vec_embedding.py'")
    print("2. Update the import in the second file to match the filename")
    print("3. Make sure both files are in the same directory")
    print("4. Run the demo functions to see output!")
    
    return driver_learner, validation_results

# Main execution
if __name__ == "__main__":
    # Run the complete demo
    driver_learner, validation_results = run_full_demo()